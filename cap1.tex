\chapter{Introdução} \label{cap:cap1}

A difusão de informações em dispositivos digitais é feita através de conteúdos audiovisuais com objetos de mídia, que são classificados em: mídias discretas e mídias contínuas. As mídias discretas são compostas por itens de informação independentes do tempo e com sua dimensão unicamente espacial, como por exemplo textos, imagens e gráficos. Já as mídias contínuas são caracterizadas por sua dependência temporal entre os itens de informação, ou seja, o tempo faz parte da semântica da informação, como por exemplo áudios, vídeos e animações \cite{halsall2001multimedia}.

Sistemas hipermídia são subdivididos em três componentes principais: armazenamento, especificação (autoria) e execução (formatação). As mídias digitais sendo elas contínuas e/ou discretas são orquestradas por esses sistemas usando os três componentes. O componente de armazenamento define como as informações contidas na mídias serão armazenadas. Já o componente de especificação diz respeito a descrição formalizada de como a aplicação hipermídia será executada. Ao se descrever essa aplicação é produzido um documento hipermídia. A execução trabalha para que tudo que foi especificado no componente de autoria possa ser executado, como por exemplo, iniciar os aplicativos necessários para a renderização das mídias, captura de eventos que ocorrem no sistema operacional do \textit{middleware}, sincronização temporal das apresentações, etc. Além disso as aplicações hipermídia podem prover a interação com usuário, ou seja, permitem definir apresentações interativas utilizando múltiplas mídias. 

Aplicações hipermídia estão disponíveis em várias plataformas como \textit{smartphones}, computadores, \textit{tablets} e TVs digitais. Essas aplicações são desenvolvidas por diversos tipos de profissionais que variam desde designers gráficos, editores audiovisuais e autores que trabalham na construção de documentos hipermídia. Enquanto designers trabalham buscando definir a qualidade e editores com a produção de determinados itens de mídia específicos da aplicação, o autor de documentos hipermídia trabalha com a integração dos módulos de modo que a aplicação atenda às especificações desejadas \cite{soares2000modeling}.

Os documentos hipermídia podem ser criados por meio de uma linguagem de autoria hipermídia, que permite especificar tanto propriedades temporais quanto espaciais das mídias que participam da aplicação. Essas características podem se apresentar de diversas maneiras tais como: o relacionamento entre duas ou mais mídias; o tempo de duração de cada uma; a posição da tela em que ela é apresentada; as interações com o usuário final da aplicação, a adaptação de conteúdo e de leiaute, entre outras \cite{soares2000modeling,barreto2016ncl}.

Diferentes linguagens hipermídia são usadas para especificar comportamentos temporais e espaciais de um documento hipermídia. Dentre as principais, estão as linguagens de autoria HTML5 (\textit{HyperText Markup Language})~\cite{W3C:2014aa}, SMIL (\textit{Synchronized Multimedia Integration Language})~\cite{ayers2001synchronized} e NCL (\textit{Nested Context Language}) \cite{ITU:2009ma}. Esta última é a linguagem adotada como padrão para desenvolvimento de programas interativos no Sistema Brasileiro de TV Digital \cite{ABNT:2011aa} e foco deste trabalho. 

No ambiente de TV Digital, considerado um sistema hipermídia, o mesmo conteúdo é transmitido para vários telespectadores que estão sintonizados em um canal. Esses telespectadores podem ter opiniões distintas sobre interação com o conteúdo apresentado, por exemplo, alguns gostam de interagir ativamente com o conteúdo, outros preferem apenas assistir sem nenhuma interação, enquanto outros simplesmente não podem interagir por possuírem necessidades especiais. Neste sentido, é interessante que o ambiente de TV ofereça diferentes possibilidades de interação aos usuários, além de poder reagir de maneira diferenciada de acordo com o usuário responsável pela interação. Assim sendo, é importante fornecer novos mecanismos nas linguagens de autoria para que os autores de conteúdo multimídia possam desenvolver aplicações que considerem novas modalidades de interação. Para a aplicação reagir de forma diferente a cada usuário, será necessário armazenar informações sobre os usuários de forma individualizada.

Considerando o avanço dos dispositivos inteligentes e sensores conectados à IoT (\emph{Internet of Things}), a incorporação desses dispositivos em  plataformas de mídia e entretenimento é uma forma interessante para expandir as possibilidades de interação do usuário. Por exemplo, alguns televisores estão sendo desenvolvidos com interface de voz para acessar funcionalidades básicas, como pesquisa por conteúdo e controle de execução das mídias\footnote{https://assistant.google.com/platforms/tv/}.

Integração de novos tipos de interação em sistemas multimídia, além das modalidades tradicionais por mouse e teclado, é um tema abordado em diferentes propostas na literatura \cite{de2011multimodal,Guedes:2016aa}. De acordo com Lima et al. \cite{de2011multimodal}, o uso da linguagem natural juntamente com o gesto pode superar as limitações da interação de apenas uma modalidade. Isto porque a combinação de fala e gesto fornece um comportamento comunicativo altamente eficiente para interagir com aplicativos em uma experiência mais transparente que as interfaces tradicionais. Assim sistemas de interação multimodal representam uma interessante ferramenta pra prover um melhor qualidade na interação à medida que se possibilita o uso de modalidade mais naturais para o usuário. Porém há de se pensar sobre as características desse sistema de interação que pode ser classificado de várias formas, dependendo se há fusão dos dados de mais de uma modalidade e se o uso de modalidades é em sequencial ou em paralelo. Em Turk et al. \cite{turk2014multimodal}, há uma discussão aprofundada sobre a classificação de sistemas multimodais. Em um sistema multimodal exclusivo, as modalidades são usadas sequencialmente e estão disponíveis separadamente, mas não integradas pelo sistema. Em um sistema multimodal alternativo, as modalidades são usadas sequencialmente, mas são integradas em algum grau. Em um sistema multimodal concorrente, as informações modais estão disponíveis em paralelo, mas separadamente (não integradas). Finalmente, em um sistema multimodal sinérgico, os modos estão disponíveis em paralelo e totalmente integrados \cite{turk2014multimodal}. 

Em geral, os sistemas multimodais disponíveis comercialmente não incluem o processamento paralelo de múltiplos modos de entrada e não utilizam uma fusão destes dados, mas, em vez disso, processam apenas uma alternativa de modo por vez \cite{furht2008encyclopedia}. Neste trabalho, também consideramos as modalidades sendo disparadas uma da cada vez, sendo assim chamado de sistema multimodal exclusivo, de acordo com a classificação apresentada em Turk et al. \cite{turk2014multimodal}. 

Além de múltiplas modalidades de interação, aplicações multiusuário têm levado a diversas evoluções na forma de desenvolvimento de aplicações multimídia. Em ambientes interativos, a adição de múltiplos usuários implica a necessidade de identificação de tais usuários para poderem interagir com o sistema. Assim como conter estruturas que armazenem informações desses usuários. Para permitir autoria dessas aplicações, a linguagem deve permitir que o autor identifique cada usuário individualmente. Porém, identificar um usuário não é simples do ponto de vista de autoria. Visto que o código gerado seria altamente acoplado ao identificador do usuário do sistema.  Outro problema é que devem ser informados especificamente quais usuários e quantos usuários. Por outro lado, perfis de usuários podem ser definidos mais facilmente pelo autor como proposto no trabalho de Guedes \cite{Guedes:2016aa}. 

Com dito anteriormente, a linguagem de autoria NCL (\textit{Nested Context Language}) \cite{soares:2011-prog-NCL} faz parte do padrão brasileiro de TV Digital e é interpretada pelo \textit{middleware} Ginga-NCL \cite{ABNT:2011aa}. A linguagem NCL, em especial, oferece suporte a interatividade e é utilizada não somente para especificação de conteúdo interativo para TV Digital \cite{Soares:2007aa}, mas também para Sistemas IPTV de acordo com a recomendação ITU-T H.761 (\textit{Internet Protocol TeleVision}) \cite{ITU:2009ma}. Em televisores, a principal modalidade de interação com o conteúdo é através do controle remoto, onde um e somente um usuário de cada vez pode navegar por menus ou selecionar botões virtuais pressionando botões do controle físico.  

A adição de novas modalidades de interação abre várias possibilidades para criação de aplicações capazes de atender um grupo de usuários e cenários que até então seriam impossíveis de serem criados, tais como usuários com necessidades especiais incapazes de manusear o controle, ou ambientes em que os usuários estão com as mãos ocupadas e precisam interagir de outra forma para direcionar a execução da aplicação. No setor de saúde, podemos imaginar aplicações multimídia sendo utilizadas para terapias alternativas onde o indivíduo que está tendo a experiência com a aplicação possa utilizar outros modos de interação que sejam mais confortáveis para ele. Faria et al \cite{farias2020memo} propõem um novo exercício cognitivo chamado, Memo-VR, que usa a tecnologia de Realidade Virtual (do inglês \textit{Virtual Reality} - VR) combinada a técnicas de interação como o escaneamento e virtualização das mãos do usuário, em uma versão do clássico jogo da memória. Como esta pesquisa encontra-se em desenvolvimento, são apresentados resultados preliminares de testes com usuários idosos. Na área de educação, pode-se utilizar modalidades de interação mais próximas dos alunos a fim de apresentar o conteúdo planejado. Zhiyong Fua e Yuyao Zhoub em \cite{fu2019research} estudam a interação docente do curso aprendizagem \textit{maker} no cenário de ensino à distância, principalmente observando a eficácia da interação multimodal no ensino a distância e as mudanças nas características corporais (linguagem, gestos e \textit{feedback}). Os resultados mostram que a interação multimodal na cena remota é benéfica para melhorar a eficiência do aprendizado. A imagem apresentada no trabalho é tridimensional, o que promove a compreensão dos alunos sobre a operação prática. A sensação de interação entre os dois lados é mais ativa, conforme dito pelos autores.


\section{Questões de Pesquisa}
\newtheorem{prob}{Questão}

A TV tem sido o meio dominante de consumo de mídia audiovisual em casa há décadas, apoiando experiências compartilhadas e atraindo o olhar e a atenção das pessoas próximas. No entanto, nos últimos anos, esse domínio foi corroído pelo advento da "exibição múltipla", em que os espectadores utilizam simultaneamente duas ou mais telas ou dispositivos. Por exemplo, na Austrália, 74\% da população com conectividade com a Internet tem duas telas (o que significa que eles usaram duas telas simultaneamente, por exemplo, usando uma TV e um telefone juntos). Em comparação, 26\% fizeram a triagem tripla (o que significa que normalmente utilizaram uma combinação de TV, telefone e \textit{tablet}/\textit{laptop}). Essa transição para o uso de várias telas ocorreu porque a tecnologia e a interface da TV não conseguiram acompanhar as demandas dos usuários \cite{mcgill2015review}. Isso não é diferente no Brasil, aplicações multimídia para TV digital aberta no sistema brasileiro não possuem interação multimodal por meio do equipamentos presentes em todas TVs. Pois o Ginga, \textit{middleware} que interpreta e executa os documentos hipermídia só oferece uma modalidade de interação através do controle remoto da TV. Pessoas com dificuldades de operar o teclado ou que simplesmente não preferem esta modalidade, não têm escolha.

Já a interface do usuário com o computador permite manipular diretamente as representações audiovisuais por meio de ações de usuários. Chamadas de GUI (\textit{Graphical User Interface}), tais interfaces seguem o paradigma WIMP (Windows, Ícones, Menus e Dispositivos apontadores) \cite{shneiderman2010designing,guedes2015specification, barnes1999graphical}. Pesquisas em interação humano-computador, no entanto, vão além do WIMP e propõem outros tipos de interfaces de usuário, interações multimodais, geralmente chamadas de Post-WIMP. Essas vêm ganhado um espaço cada vez maior na interface homem-máquina \cite{jalal2017iot, waltl2010increasing}. Em particular, os avanços nas tecnologias de reconhecimento, como reconhecimento de fala ou gesto, deram origem a um tipo de interface de usuário pós-WIMP denominada MUI (\textit{Multimodal User Interface}, interface de usuário multimodal). Segundo \cite{turk2014multimodal, kopp2006towards}, as MUIs processam duas ou mais modalidades combinadas de entrada do usuário (por exemplo, fala, toque, gesto, olhar e movimentos da cabeça e do corpo) de maneira coordenada com as modalidades de saída. Uma modalidade de entrada corresponde a uma informação gerada pelo usuário capturada por dispositivos de entrada (por exemplo, microfone) ou sensores (por exemplo, sensor de movimento). Uma modalidade de saída corresponde a um estímulo para os sentidos humanos (audição, olfato, tato, paladar ou visão) usando dispositivos audiovisuais ou atuadores (por exemplo, emissor de cheiro). Desta forma as interações podem ser estabelecidas utilizando outros dispositivos capazes de capturar formas diferentes de comunicação com a aplicação como, por exemplo, reconhecimento de voz, gestos e face atendendo inclusive a vários tipos de necessidades especiais.

Aplicações multimídia são geralmente definidas com uma linguagem específica, chamada de \emph{linguagem de autoria multimídia}. Tais linguagens concentram-se na definição das mídias que farão parte da aplicação, a forma como serão apresentadas e sua sincronização ao longo do tempo \cite{Blakowski:1996aa,Hardman:1998zj}. Um subconjunto dessas linguagens, geralmente baseado em XML, usa uma abordagem declarativa fornecendo construções de alto nível de abstração para definição de aplicações multimídia. Um dos princípios que norteiam tais linguagens é proporcionar uma clara separação entre a descrição da aplicação e sua implementação, \textit{i.e.}, sua execução \cite{Hardman:1998zj}. Assim as evoluções futuras da forma de como uma aplicação é apresentada não exigem uma redefinição, ou alteração, de toda a base das aplicações previamente especificadas. Um outro fator positivo das linguagens de autoria multimídia declarativas é que elas permitem maior facilidade para criação de aplicações por autores que não são programadores \cite{soares2009programando}. 

O aumento drástico do número de dispositivos inteligentes e sensores conectados à IoT (\emph{Internet of Things}) tem o potencial de mudar a forma como os consumidores interagem com a tecnologia em rede, incluindo plataformas de mídia, entretenimento e salas terapêuticas. Isso representa uma oportunidade interessante para sistemas multimídia a fim de expandir possibilidades de interação com o usuário. Neste cenário, é possível criar aplicações mais responsivas e interativas, redefinindo o nível de interação entre os ambientes das aplicações e seus usuários \cite{jalal2017iot}, criando novas demandas. Assim se torna interessante prover para o autor de conteúdo novas construções nas linguagens de autoria declarativas para que ele possa desenvolver aplicações multimídia. Portanto considera-se que a construção de uma linguagem declarativa capaz de expressar interações multiusuário e multimodais possa avançar o estado da arte.

Esta tese de doutorado define as seguintes questões de pesquisa a serem estudadas e solucionadas por este trabalho.

\begin{prob}
\label{prob:pMultimodal}
    As aplicações multimídias para TV possibilitam interações dos usuários por várias modalidades além do controle remoto de maneira nativa? Atendendo assim várias classes de usuários  e necessidades?
    Os usuários podem utilizar diversos modos ou dispositivos de interação com a aplicação?
\end{prob}

Um dos casos de uso que ilustra a utilidade da interação multimodal é uma aplicação que possibilita a interação com a TV por meio da interação de voz ou gestos para pessoas que não possui o controle das mãos para usar o controle remoto da TV. Outra possibilidade é capturar interações por meio de reconhecimento de expressões faciais e assim a aplicação mudar seu comportamento.

 \begin{prob}
\label{prob:pMultiUsuario}
    As aplicações multimídia para TV digital não são capazes de identificar provedor da interação. Os dispositivo de interação atualmente usado não possibilita a identificar quem esta interagindo. Mesmo que há dispositivos capazes de identificar o "dono" da interação, não existe um arquitetura nativa capaz de gerenciar os usuários e suas propriedades de forma a ser utilizados nas aplicações multimídia.
\end{prob}

Em aplicações multimídia em que pode existir vários usuários participando da experiência mas esta aplicação só pode responder para um grupo específico de usuários com determinadas propriedades. Assim, o \textit{middleware} só irá responder a interações destes usuários. O formatador estaria identificando os usuários e suas interações configurando um ambiente multiusuário. Além disso, poderá captar informações de diferentes usuários que estão assistindo o conteúdo e fazer uma análise ou até mesmo disparar eventos diferentes. 

Além disso, o \textit{middleware} não contempla um interação multiusuário identificando suas interações individualmente e permitindo que aplicação mude sua execução de acordo com essas interações. Considerando um interação multiusuário, um fator importante é a contextualização das informações de cada usuário que participa da experiência. Contexto esse que pode ser estendido para o ambiente em que se executa a aplicação. Alguma característica do ambiente pode ser usada para direcionar a aplicação de acordo com as preferências ou limitações do usuário. Para isso essas informações devem estar disponíveis no momento execução do documento. Atualmente o \textit{middleware} Ginga não possui um estrutura que possibilite armazenar informações dos usuários de maneira individualizada. Desta forma, é interessante a criação de grupos de dados que tenham significados relacionados. Por exemplo, se torna interessante agrupar informações dos pacientes em um contexto e as do terapeuta em outro, já que há mais de um usuário de perfis diferentes. Possibilitar carregar informações sobre o perfil dos usuários que estão participando da experiência.


\section{Objetivos}
\label{sec:objetivos}

Esta tese tem o seguinte objetivo geral:

Contribuir para autoria declarativa de aplicações hipermídia com novas facilidades que permitam vários usuários interagirem com a aplicação, de maneira individualizada, e forneçam suporte a interação multimodal em um ambiente multimídia.

Como objetivos específicos, este trabalho propõe uma extensão ao modelo NCM (\textit{Nested Context Model}) \cite{Soares:2005qy} e à linguagem de autoria NCL, que implementa o modelo, a fim de possibilitar a descrição de novas aplicações multimídia, com suporte a multiusuário e interação multimodal. Com a extensão proposta neste trabalho, a linguagem NCL tem potencial para ser usada para a criação de aplicações que possam ser executadas em vários ambientes multimídia interativos, como por exemplo museus interativos ou salas de terapia multimídia. NCM e NCL foram escolhidos como modelo e linguagem base para este trabalho, pois NCM utiliza um modelo de sincronização baseado em eventos, sendo possível contemplar eventos assíncronos como por exemplo de interação com usuário. Além disso, modelo permite extensão do conjunto de eventos de interação e o armazenamento de variáveis do documento. A linguagem NCL, além de implementar as entidades criadas no NCM, segue o paradigma declarativo, cuja implementação de documentos hipermídia se torna mais acessível para autores não programadores. 

A extensão da linguagem é feita propondo um novo módulo de linguagem e alterando vários existentes. A implementação de suporte multiusuário é proposta com a criação de uma nova área funcional \textit{Users}, um novo módulo chamado \textit{User}, extensão do módulo \textit{CausalConnectorFunctionality} e extensão do módulo  \textit{media}. 

A proposta é avaliada de forma quantitativa. A proposta foi implementada no \textit{middleware} Ginga-NCL, propondo uma arquitetura que permite a integração de diferentes dispositivos de interação ao \textit{middleware} em um ambiente multiusuário. O desempenho da implementação é avaliada para interação por voz, sendo comparada à versão padrão do Ginga-NCL \cite{abnt-sbtvd} com uso de código procedural Lua. Além do reconhecimento de voz, também foi implementado o módulo de interação por meio da fixação do olhar, reconhecimento de expressões faciais e gestos. Foram criadas aplicações de teste usando dois modos de interação complementares, como uma prova de conceito e para avaliar o novo sistema multimodal. Além disso, vários testes foram conduzidos para avaliar a capacidade de capturar eventos de interação em vários intervalos de tempo e para avaliar como os módulos impactam o \textit{middleware}, medindo o atraso adicionado ao integrar os novos modos de interação. Para avaliar arquitetura multiusuários, vários experimentos foram realizados medindo o tempo de verificação dos usuários de um determinado perfil, além da medição do atraso gasto na criação de links de interação individualizado para cada usuário.


\section{Contribuições da Tese}
\label{sec:contribuicoes}

 

Esta tese avança o estado da arte em modelos conceituais e linguagens de autoria declarativa pois propõe a utilização de novos tipos de evento para representar interação multimodal. A proposta foi implementada no modelo NCM e na linguagem NCL para que possam especificar vários tipos de interação do usuário. Foi realizada uma extensão da definição em \textit{XML Schema} de NCL acrescentando outros eventos de interação no elemento \textit{<simpleCondition>}, que faz parte do módulo \textit{CausalConnectorFunctionality}. 

Outra contribuição importante é a modificação do \textit{middleware} Ginga-NCL para prover as novas funcionalidades de NCL, viabilizando o tratamento de várias modalidades de interação com o usuário. Foi implementado um novo módulo no \textit{middleware} chamado \textit{InteractionManager}, que gerencia as várias modalidades de interação utilizadas pela aplicação multimídia. Isso é possível porque foi acrescentado ao elemento \textit{simpleCondition} outros papeis que representam novas modalidade de interação além do apertar de teclas do controle remoto ou teclado.

A possibilidade de permitir a participação de vários usuários na experiência multimídia também é uma contribuição deste trabalho. O resultado disso foi a especificação da área funcional \textit{User} e do módulo \textit{User} contendo os elementos \textit{useAgent} e \textit{userProfile} no \textit{schema} NCL. E a modificação do Ginga-NCL para que o \textit{middleware} possa validar e tomar ações diferentes quando a interação de uma determinada modalidade vier identificada com o usuário que interagiu. Mais uma contribuição foi a adição de um novo tipo para o elemento \textit{<media>} tanto no esquema quanto na implementação do \textit{middleware} alterando o módulo \textit{<Media>}. Esse tipo é o \textit{userSettingsNode}. No modelo NCM ele foi especificado como a entidade \textit{UserSettingsNode} que estende a entidade \textit{SettingsNode}. Isso foi feito para possibilitar às aplicações multimídia armazenarem informações dos usuários em contextos diferentes e em nós diferentes já que nessa nova versão é possível existir várias instâncias de \textit{UserSettingsNode} diferente da versão 3.0 de NCL que permite somente uma instância do nó de informações de contexto por aplicação. Outra contribuição é a implementação de um parser para carregar as informações do usuários nesses nós de conteúdo do tipo SettingsNode. Como prova de conceito, o parser implementado carrega informações armazenadas em um arquivo XML seguindo o padrão MPEG-21 \cite{ISO/IEC:2019aa}. 

A avaliação realizada para diferentes modalidades de interação também é uma contribuição desta tese. A avaliação foi divida em etapas: comparação com Ginga-NCL atual usando \textit{script} NCLua; capacidade de tratamento de eventos de interação em diversos intervalos de tempos; influência do tratamento do eventos na ações do \textit{middleware}; medição do atraso por causa da identificação dos usuários e criação de links individuais de interação. E por fim, estudo de quatro casos de uso para aplicabilidade de multimodalidade e multiusuário em TV Digital. 


\section{Estrutura da Tese}
\label{sec:estrutura}

O restante do texto está estruturado da seguinte forma. O Capítulo~\ref{cap:cap2} comenta os principais conceitos de ambientes hipermídia, apresentando várias abordagens desenvolvidas na decorrer da história e apresentando e descrevendo os diversos modelos propostos, incluindo o modelo NCM, que foi detalhado descrevendo suas entidades e sua expressividade. Esse capítulo também apresenta os módulos e os principais elementos da linguagem NCL com suas funcionalidades, além de exemplificar aplicações NCL.

O Capítulo~\ref{cap:cap3} contém os trabalhos relacionados a esta tese assim como o posicionamento deste trabalho frente aos demais.

No Capítulo~\ref{cap:cap4} é apresentada a proposta de extensão ao modelo NCM apresentando as novas entidades assim como alterações nas entidades existentes. O capítulo também apresenta as entidades alteradas ou criadas para atender a cada uma das funcionalidades apresentadas neste trabalho considerando interação multimodal e multiusuário. De maneira semelhante, é feito em com a linguagem NCL, ou seja, todos os módulos e elementos alterados ou criados foram detalhados no capítulo. 

O Capítulo~\ref{cap:cap5} apresenta a implementação da proposta feita no Ginga-NCL, mostrando inicialmente a arquitetura idealizada. Em seguida, o capítulo comenta os detalhes de implementação da arquitetura na nova versão do \textit{middleware} Ginga-NCL. 

O Capítulo~\ref{cap:cap6} apresenta uma avaliação das extensões propostas. O capítulo apresenta as três etapas de avaliação de proposta implementada de multimodalidade: a comparação usando um aplicação NCL feita para o Ginga atual e uma aplicação para o Ginga estendido; teste de carga da capacidade de captura de eventos multimodais; teste de carga para avaliar se o tratamento dos eventos multimodais afeta a execução de outras ações do Ginga estendido. Além disso, o capítulo apresenta uma avaliação da proposta de multiusuário em duas partes, a primeira um medição do tempo gasto em carregar e avaliar o perfil do usuário e a segunda mede o tempo gasto com a criação dos linsk individuais.

O Capítulo~\ref{cap:cap7} apresenta vários casos de usu em TV Digital em um cenário multimodal e multiusuário.

Por fim, o Capítulo~\ref{cap:cap8} conclui o trabalho, apresentando as conclusões, realçando as contribuições desta tese e apontando os  trabalhos futuros.